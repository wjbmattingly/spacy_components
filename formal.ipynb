{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vocal-tracker",
   "metadata": {},
   "source": [
    "# Adding Custom Components and Factories to spaCy and Packaging Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "heavy-bidder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-flashing",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "blank-ending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/2xi9SvgFLks\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></center>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/2xi9SvgFLks\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-bowling",
   "metadata": {},
   "source": [
    "## Creating the Custom Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-times",
   "metadata": {},
   "source": [
    "Here we will add our custom component. In spaCy there are two ways to do this, either as a class or as a function. In the case of classes, you will make a **factory**. In the case of functions, you will make a **component**. Let's take each in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fitting-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function for removing PERSON from our output\n",
    "@Language.component(\"remove_person\")\n",
    "def remove_person(doc):\n",
    "    final = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ != \"PERSON\":\n",
    "            final.append(ent)\n",
    "    doc.ents = final\n",
    "    return (doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-colon",
   "metadata": {},
   "source": [
    "## Loading the Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-peter",
   "metadata": {},
   "source": [
    "Now that we have our component ready, let's bring in a model that we intend to add it to. In our case, we will add it to the spaCy small English model, so let's load it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "laden-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-behalf",
   "metadata": {},
   "source": [
    "Let's see what it looks like if we try and extract entities from this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "integral-gossip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom PERSON\n",
      "Jerry PERSON\n",
      "USA GPE\n"
     ]
    }
   ],
   "source": [
    "text = \"Tom and Jerry are not friends, but they do run around a lot together in the USA.\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-offering",
   "metadata": {},
   "source": [
    "Everything is as expected. Now, let's try and add our custom component to the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-member",
   "metadata": {},
   "source": [
    "## Adding Component to Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-postcard",
   "metadata": {},
   "source": [
    "Now that we have the model loaded, let's add our new component to the end of the pipeline. We want it to sit after the NER because our goal is to adjust the NER output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "systematic-strength",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.remove_person(doc)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"remove_person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-entrepreneur",
   "metadata": {},
   "source": [
    "## Analyze the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "motivated-delight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'remove_person': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'ner': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'remove_person': []},\n",
       " 'attrs': {'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-breed",
   "metadata": {},
   "source": [
    "## Testing the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recognized-minutes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA GPE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-despite",
   "metadata": {},
   "source": [
    "Viola. Our pipe is working. If we were to save this model and try and use it in another script, however, it would break. We would need to have our custom component in that script so that spaCy knew how to handle that pipe. This becomes highly problematic when we try and package and distribute our models in production. Let's work through how to solve that problem now. Let's go ahead and save it to disk anyway for the next stage of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crude-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"test_pipe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-processing",
   "metadata": {},
   "source": [
    "## Packaging Model with Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-allowance",
   "metadata": {},
   "source": [
    "First we need to make a Python file that has the component inside of it. You can see this file in the repo entitled \"my_component.py\" Once that file is created we can call it in our packaging command in the terminal. Because this is in a Jupyter Notebook, we can do this with an \"!\" in the cell below. The command works like this:\n",
    "1) We call Python first so the terminal knows to execute a Python code.<br>\n",
    "2) Next we specify that we are calling the spaCy library and its set of terminal commands.<br>\n",
    "3) We state that we want to run package from spaCy. This is the command to package a model.<br>\n",
    "4) We then specify the directory in which we want to drop the packaged model. In this case, \"compiled\".<br>\n",
    "5) Finally, we tell package that this model is special. It takes a keyword argument --code followed by what file the code is in. In this case, my_component.py. Now, the spaCy model has had the code packaged with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "complex-assignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "creating en_core_web_sm.egg-info\n",
      "writing en_core_web_sm.egg-info\\PKG-INFO\n",
      "writing dependency_links to en_core_web_sm.egg-info\\dependency_links.txt\n",
      "writing entry points to en_core_web_sm.egg-info\\entry_points.txt\n",
      "writing requirements to en_core_web_sm.egg-info\\requires.txt\n",
      "writing top-level names to en_core_web_sm.egg-info\\top_level.txt\n",
      "writing manifest file 'en_core_web_sm.egg-info\\SOURCES.txt'\n",
      "reading manifest file 'en_core_web_sm.egg-info\\SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'en_core_web_sm.egg-info\\SOURCES.txt'\n",
      "running check\n",
      "creating en_core_web_sm-3.0.0\n",
      "creating en_core_web_sm-3.0.0\\en_core_web_sm\n",
      "creating en_core_web_sm-3.0.0\\en_core_web_sm.egg-info\n",
      "creating en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\n",
      "creating en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\attribute_ruler\n",
      "creating en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\lemmatizer\n",
      "creating en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\lemmatizer\\lookups\n",
      "creating en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\ner\n",
      "creating en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\parser\n",
      "creating en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\senter\n",
      "creating en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\tagger\n",
      "creating en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\tok2vec\n",
      "creating en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\vocab\n",
      "copying files to en_core_web_sm-3.0.0...\n",
      "copying MANIFEST.in -> en_core_web_sm-3.0.0\n",
      "copying meta.json -> en_core_web_sm-3.0.0\n",
      "copying setup.py -> en_core_web_sm-3.0.0\n",
      "copying en_core_web_sm\\__init__.py -> en_core_web_sm-3.0.0\\en_core_web_sm\n",
      "copying en_core_web_sm\\meta.json -> en_core_web_sm-3.0.0\\en_core_web_sm\n",
      "copying en_core_web_sm\\my_component.py -> en_core_web_sm-3.0.0\\en_core_web_sm\n",
      "copying en_core_web_sm.egg-info\\PKG-INFO -> en_core_web_sm-3.0.0\\en_core_web_sm.egg-info\n",
      "copying en_core_web_sm.egg-info\\SOURCES.txt -> en_core_web_sm-3.0.0\\en_core_web_sm.egg-info\n",
      "copying en_core_web_sm.egg-info\\dependency_links.txt -> en_core_web_sm-3.0.0\\en_core_web_sm.egg-info\n",
      "copying en_core_web_sm.egg-info\\entry_points.txt -> en_core_web_sm-3.0.0\\en_core_web_sm.egg-info\n",
      "copying en_core_web_sm.egg-info\\not-zip-safe -> en_core_web_sm-3.0.0\\en_core_web_sm.egg-info\n",
      "copying en_core_web_sm.egg-info\\requires.txt -> en_core_web_sm-3.0.0\\en_core_web_sm.egg-info\n",
      "copying en_core_web_sm.egg-info\\top_level.txt -> en_core_web_sm-3.0.0\\en_core_web_sm.egg-info\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\config.cfg -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\meta.json -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\tokenizer -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\attribute_ruler\\patterns -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\attribute_ruler\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\lemmatizer\\lookups\\lookups.bin -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\lemmatizer\\lookups\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\ner\\cfg -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\ner\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\ner\\model -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\ner\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\ner\\moves -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\ner\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\parser\\cfg -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\parser\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\parser\\model -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\parser\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\parser\\moves -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\parser\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\senter\\cfg -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\senter\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\senter\\model -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\senter\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\tagger\\cfg -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\tagger\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\tagger\\model -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\tagger\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\tok2vec\\cfg -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\tok2vec\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\tok2vec\\model -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\tok2vec\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\vocab\\key2row -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\vocab\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\vocab\\lookups.bin -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\vocab\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\vocab\\strings.json -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\vocab\n",
      "copying en_core_web_sm\\en_core_web_sm-3.0.0\\vocab\\vectors -> en_core_web_sm-3.0.0\\en_core_web_sm\\en_core_web_sm-3.0.0\\vocab\n",
      "Writing en_core_web_sm-3.0.0\\setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'en_core_web_sm-3.0.0' (and everything under it)\n",
      "[i] Building package artifacts: sdist\n",
      "[+] Including 1 Python module(s) with custom code\n",
      "[+] Loaded meta.json from file\n",
      "test_pipe\\meta.json\n",
      "[+] Successfully created package 'en_core_web_sm-3.0.0'\n",
      "compiled\\en_core_web_sm-3.0.0\n",
      "[+] Successfully created zipped Python package\n",
      "compiled\\en_core_web_sm-3.0.0\\dist\\en_core_web_sm-3.0.0.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-03 09:06:56.082096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\n",
      "warning: no files found matching 'LICENSE'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy package test_pipe compiled --code my_component.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-glossary",
   "metadata": {},
   "source": [
    "In spaCy 2, we used to have to compile the tar.gz, but not anymore with spaCy 3. If you look in the dist folder of the model in compiled, you will notice that the tar.gz is there. This tar.gz contains the code for your custom component. You can pip install the tar.gz like any other model. Just make sure that you pip install the whole name, including the tar.gz. Once installed, that model will know how to handle your custom component perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-going",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
